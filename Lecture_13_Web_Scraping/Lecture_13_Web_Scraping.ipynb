{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python Using Beautiful Soup\n",
    "\n",
    "The internet is an absolutely massive source of data — data that we can access using web scraping and Python!\n",
    "\n",
    "In fact, web scraping is often the only way we can access data. There is a lot of information out there that isn’t available in convenient CSV exports or easy-to-connect APIs. And websites themselves are often valuable sources of data — consider, for example, the kinds of analysis you could do if you could download every post on a web forum.\n",
    "\n",
    "To access those sorts of on-page datasets, we’ll have to use web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fundamentals of Web Scraping:\n",
    "### What is Web Scraping in Python?\n",
    "Some websites offer data sets that are downloadable in CSV format, or accessible via an Application Programming Interface (API). But many websites with useful data don’t offer these convenient options.\n",
    "\n",
    "If we wanted to analyze the data from a website, or download it for use in some other app, we wouldn’t want to painstakingly copy-paste everything. Web scraping is a technique that lets us use programming to do the heavy lifting.\n",
    "\n",
    "### How Does Web Scraping Work?\n",
    "When we scrape the web, we write code that sends a request to the server that’s hosting the page we specified. The server will return the source code — HTML, mostly — for the page (or pages) we requested.\n",
    "\n",
    "So far, we’re essentially doing the same thing a web browser does — sending a server request with a specific URL and asking the server to return the code for that page.\n",
    "\n",
    "But unlike a web browser, our web scraping code won’t interpret the page’s source code and display the page visually. Instead, we’ll write some custom code that filters through the page’s source code looking for specific elements we’ve specified, and extracting whatever content we’ve instructed it to extract.\n",
    "\n",
    "For example, if we wanted to get all of the data from inside a table that was displayed on a web page, our code would be written to go through these steps in sequence:\n",
    "\n",
    "- Request the content (source code) of a specific URL from the server\n",
    "\n",
    "- Download the content that is returned\n",
    "\n",
    "- Identify the elements of the page that are part of the table we want\n",
    "\n",
    "- Extract and (if necessary) reformat those elements into a dataset we can analyze or use in whatever way we require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Components of a Web Page\n",
    "\n",
    "\n",
    "Before we start writing code, we need to understand a little bit about the structure of a web page. We’ll use the site’s structure to write code that gets us the data we want to scrape, so understanding that structure is an important first step for any web scraping project.\n",
    "When we visit a web page, our web browser makes a request to a web server. This request is called a GET request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. These files will typically include:\n",
    "\n",
    "- HTML — the main content of the page.\n",
    "\n",
    "- CSS — used to add styling to make the page look nicer.\n",
    "\n",
    "- JS — Javascript files add interactivity to web pages.\n",
    "\n",
    "- Images — image formats, such as JPG and PNG, allow web pages to show pictures.\n",
    "\n",
    "\n",
    "After our browser receives all the files, it renders the page and displays it to us.\n",
    "\n",
    "There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look primarily at the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "\n",
    "HyperText Markup Language (HTML) is the language that web pages are created in. HTML isn’t a programming language, like Python, though. It’s a markup language that tells a browser how to display content.\n",
    "\n",
    "HTML has many functions that are similar to what you might find in a word processor like Microsoft Word — it can make text bold, create paragraphs, and so on.\n",
    "\n",
    "If you’re already familiar with HTML, feel free to jump to the next section of this tutorial. Otherwise, let’s take a quick tour through HTML so we know enough to scrape effectively.\n",
    "\n",
    "HTML consists of elements called tags. The most basic tag is the `<html>` tag. This tag tells the web browser that everything inside of it is HTML. We can make a simple HTML document just using this tag:\n",
    "\n",
    "\n",
    "```html\n",
    "<html>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "We haven’t added any content to our page yet, so if we viewed our HTML document in a web browser, we wouldn’t see anything:\n",
    "\n",
    "Right inside an `html` tag, we can put two other tags: the `head` tag, and the `body` tag.\n",
    "\n",
    "The main content of the web page goes into the body tag. The head tag contains data about the title of the page, and other information that generally isn’t useful in web scraping:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "We still haven’t added any content to our page (that goes inside the body tag), so if we open this HTML file in a browser, we still won’t see anything:\n",
    "\n",
    "You may have noticed above that we put the head and body tags inside the html tag. In HTML, tags are nested, and can go inside other tags.\n",
    "\n",
    "We’ll now add our first content to the page, inside a p tag. The p tag defines a paragraph, and any text inside the tag is shown as a separate paragraph:\n",
    "\n",
    "\n",
    "```html\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "<p>\n",
    "Here's a paragraph of text!\n",
    "</p>\n",
    "<p>\n",
    "Here's a second paragraph of text!\n",
    "</p>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Rendered in a browser, that HTML file will look like this:\n",
    "\n",
    "Here’s a paragraph of text!\n",
    "\n",
    "Here’s a second paragraph of text!\n",
    "\n",
    "Tags have commonly used names that depend on their position in relation to other tags:\n",
    "\n",
    "- child — a child is a tag inside another tag. So the two p tags above are both children of the body tag.\n",
    "\n",
    "- parent — a parent is the tag another tag is inside. Above, the html tag is the parent of the body tag.\n",
    "\n",
    "- sibling — a sibling is a tag that is nested inside the same parent as another tag. For example, head and body are siblings, since they’re both inside html. Both p tags are siblings, since they’re both inside body.\n",
    "\n",
    "\n",
    "We can also add properties to HTML tags that change their behavior. Below, we’ll add some extra text and hyperlinks using the a tag.\n",
    "\n",
    "```html\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "<p>\n",
    "Here's a paragraph of text!\n",
    "<a href=\"https://www.uni-osnabrueck.de/\">Uni Osnabrueck</a>\n",
    "</p>\n",
    "<p>\n",
    "Here's a second paragraph of text!\n",
    "<a href=\"https://www.python.org\">Python</a> </p>\n",
    "</body></html>\n",
    "```\n",
    "\n",
    "\n",
    "Here’s how this will look:\n",
    "\n",
    "Here’s a paragraph of text! Uni Osnabrueck\n",
    "\n",
    "Here’s a second paragraph of text! Python\n",
    "\n",
    "In the above example, we added two a tags. a tags are links, and tell the browser to render a link to another web page. The href property of the tag determines where the link goes.\n",
    "\n",
    "a and p are extremely common html tags. Here are a few others:\n",
    "\n",
    "- div — indicates a division, or area, of the page.\n",
    "\n",
    "- b — bolds any text inside.\n",
    "\n",
    "- i — italicizes any text inside.\n",
    "\n",
    "- table — creates a table.\n",
    "\n",
    "- form — creates an input form.\n",
    "\n",
    "\n",
    "For a full list of tags, visit [HTML Tags](https://www.w3schools.com/tags/default.asp).\n",
    "\n",
    "\n",
    "Note: Even though there are lots of them, most of them are rarely used.\n",
    "\n",
    "\n",
    "Before we move into actual web scraping, let’s learn about the `class` and `id` properties. These special properties give HTML elements names, and make them easier to interact with when we’re scraping.\n",
    "\n",
    "One element can have multiple classes, and a class can be shared between elements. Each element can only have one id, and an id can only be used once on a page. Classes and ids are optional, and not all elements will have them.\n",
    "\n",
    "We can add classes and ids to our example:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "<p>\n",
    "Here's a paragraph of text!\n",
    "<a href=\"https://www.uni-osnabrueck.de/\" id=\"learn-link\">Uni Osnabrueck</a>\n",
    "</p>\n",
    "<p>\n",
    "Here's a second paragraph of text!\n",
    "<a href=\"https://www.python.org\" class=\"extra-large bold\">Python</a> </p>\n",
    "</body></html>\n",
    "```\n",
    "\n",
    "Here’s how this will look:\n",
    "\n",
    "Here’s a paragraph of text! Uni Osnabrueck\n",
    "\n",
    "Here’s a second paragraph of text! Python\n",
    "\n",
    "As you can see, adding classes and ids doesn’t change how the tags are rendered at all. They are called selectors and used for selecting the tags to apply styling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The requests library\n",
    "\n",
    "The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python requests library.\n",
    "\n",
    "The requests library will make a `GET` request to a web server, which will download the HTML contents of a given web page for us. There are several different types of requests we can make using requests, of which GET is just one.\n",
    "\n",
    "Let’s try downloading a simple sample website,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://webscrapingtutorial.netlify.app/simple.html\"\n",
    "headers = {\"Accept-Language\": \"en-US\"}\n",
    "# headers are not mandatory but some websites will change the page language depending on from where you connect.\n",
    "# setting Accept-Language to en-US will make sure that the page is in english.\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A status_code of `200` means that the page downloaded successfully. We won’t fully dive into status codes here, but a status code starting with a 2 generally indicates success, and a code starting with a 4 or a 5 indicates an error.\n",
    "\n",
    "[Check more on http status codes (optional)](https://http.cat)\n",
    "\n",
    "\n",
    "We can print out the HTML content of the page using the content property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html>\\n    <head>\\n        <title>A simple example page</title>\\n    </head>\\n    <body>\\n        <p>Here is some simple content for this page.</p>\\n        <p>Here is some more text.</p>\\n    </body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "print(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a page with BeautifulSoup\n",
    "\n",
    "As you can see above, we now have downloaded an HTML document.\n",
    "\n",
    "We can use the BeautifulSoup library to parse this document, and extract the text from the p tag.\n",
    "\n",
    "We first have to import the library, and create an instance of the BeautifulSoup class to parse our document:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   Here is some simple content for this page.\n",
      "  </p>\n",
      "  <p>\n",
      "   Here is some more text.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# BeautifulSoup expects the html code and the parser. \n",
    "# lxml can be also use as parser instead of html.parser\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all instances of a tag at once\n",
    "\n",
    "If we want to extract a single tag, we can instead use the find_all method, which will find all the instances of a tag on a page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Here is some simple content for this page.</p>,\n",
       " <p>Here is some more text.</p>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that find_all returns a list, so we’ll have to loop through, or use list indexing, it to extract text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Here is some more text.</p>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is some more text.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the text instead of the html code we can extract it using .text or .get_text()\n",
    "soup.find_all('p')[1].text\n",
    "soup.find_all('p')[1].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you instead only want to find the first instance of a tag, you can use the find method, which will return a single BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Here is some simple content for this page.</p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for tags by class and id\n",
    "\n",
    "We introduced classes and ids earlier, but it probably wasn’t clear why they were useful.\n",
    "\n",
    "Classes and ids are used by CSS to determine which HTML elements to apply certain styles to. But when we’re scraping, we can also use them to specify the elements we want to scrape.\n",
    "\n",
    "To illustrate this principle, we’ll work with the following page:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "        <title>A simple example page</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <p class=\"inner-text first-item\" id=\"first\">\n",
    "                First paragraph.\n",
    "            </p>\n",
    "            <p class=\"inner-text\">\n",
    "                Second paragraph.\n",
    "            </p>\n",
    "        </div>\n",
    "            <p class=\"outer-text first-item\" id=\"second\">\n",
    "                <b>\n",
    "                First outer paragraph.\n",
    "                </b>\n",
    "            </p>\n",
    "            <p class=\"outer-text\">\n",
    "                <b>\n",
    "                Second outer paragraph.\n",
    "                </b>\n",
    "            </p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "Let’s first download the page and create a BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>A simple example page</title>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                First paragraph.\n",
       "            </p>\n",
       "<p class=\"inner-text\">\n",
       "                Second paragraph.\n",
       "            </p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                First outer paragraph.\n",
       "                </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                Second outer paragraph.\n",
       "                </b>\n",
       "</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(\"https://webscrapingtutorial.netlify.app/ids_and_classes.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `find_all` method to search for items by class or by id. In the below example, we’ll search for any p tag that has the class outer-text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 First paragraph.\n",
       "             </p>,\n",
       " <p class=\"inner-text\">\n",
       "                 Second paragraph.\n",
       "             </p>,\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 First outer paragraph.\n",
       "                 </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 Second outer paragraph.\n",
       "                 </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "soup.find_all(re.compile(r'p|(b\\s)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(string=re.compile(r'\\s(\\w)* outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on NoneType object:\n",
      "\n",
      "class NoneType(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      True if self else False\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(soup.findall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st try:\n",
      "[]\n",
      "\n",
      "2nd try:\n",
      "[<p class=\"inner-text first-item\" id=\"first\">\n",
      "                First paragraph.\n",
      "            </p>, <p class=\"inner-text\">\n",
      "                Second paragraph.\n",
      "            </p>, <p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                First outer paragraph.\n",
      "                </b>\n",
      "</p>, <p class=\"outer-text\">\n",
      "<b>\n",
      "                Second outer paragraph.\n",
      "                </b>\n",
      "</p>]\n"
     ]
    }
   ],
   "source": [
    "print(\"1st try:\", soup.find_all(\"p\", \"-text\"), \"\", sep=\"\\n\")\n",
    "print(\"2nd try:\", soup.find_all(\"p\", re.compile(\"-text\")), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "b\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(re.compile(\"b\")):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(re.compile(r'o.{3}r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 First outer paragraph.\n",
       "                 </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 Second outer paragraph.\n",
       "                 </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find all p tags which has outer-text class\n",
    "soup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 First outer paragraph.\n",
       "                 </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find all tags which has outer-text and first-item classes\n",
    "soup.find_all(class_=\"outer-text first-item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 First paragraph.\n",
       "             </p>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also search for elements by id:\n",
    "soup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 First outer paragraph.\n",
       "                 </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or we can pass an object to select multiple attributes\n",
    "soup.find_all('p', {'class': 'outer-text first-item', 'id': 'second'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CSS Selectors\n",
    "\n",
    "We can also search for items using CSS selectors. These selectors are how the CSS language allows developers to specify HTML tags to style. Here are some examples:\n",
    "\n",
    "- p a — finds all a tags inside of a p tag.\n",
    "\n",
    "- body p a — finds all a tags inside of a p tag inside of a body tag.\n",
    "\n",
    "- html body — finds all body tags inside of an html tag.\n",
    "\n",
    "- p.outer-text — finds all p tags with a class of outer-text.\n",
    "\n",
    "- p#first — finds all p tags with an id of first.\n",
    "\n",
    "- body p.outer-text — finds any p tags with a class of outer-text inside of a body tag.\n",
    "\n",
    "[Check more on CSS selectors](https://www.w3schools.com/css/css_selectors.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 First paragraph.\n",
       "             </p>,\n",
       " <p class=\"inner-text\">\n",
       "                 Second paragraph.\n",
       "             </p>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"div p\")\n",
    "#Note that the select method above returns a list of BeautifulSoup objects, just like find and find_all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN0MjfnMRbVM"
   },
   "source": [
    "## Regex\n",
    "Regular expressions are an important tool for webscraping. You will need to search for patterns of strings in HTML documents, for instance for certain tags. Before we start using regex in beautiful soup, we will go over a few basics. In the depicted examples the first chapter of \"Harry Potter and the Philosopher's Stone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "Y8sby-tf0YbF",
    "outputId": "5f2a221b-fdb7-4559-81f1-bebcf84b8d07"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "with open(\"Potter1.1.txt\", \"rt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all adverbs (and Lily)\n",
    "re.findall(r\"\\w+ly\\b\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Harry's aunt called again? It's Mrs. something\n",
    "m = re.search(r\"Mrs. (\\w+)\", text)\n",
    "print(\"The whole match:\", m[0], sep=\"\\n\")\n",
    "print(\"\\nJust the name, i.e. the first group of the match:\", m[1], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHwf4n9yznDv",
    "tags": []
   },
   "source": [
    "### Regex Syntax\n",
    "Before we can query for text passages in some string, we should create and compile a regular expression. A compiled regex will be a ``Pattern`` object. Further down the line you will see, that you often won't need to explicitly compile regular expressions, but the functions you will use (e.g. in beautiful soup) does that for you. Nevertheless in the following cells the explicit way is shown for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a match can be checked with an if-statement on a `Match` object\n",
    "def print_result(match):\n",
    "    if match:\n",
    "        print(\"It's a match!\")\n",
    "    else:\n",
    "        print(\"No match :'(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this regular expression will \"match\" any string, that consists of lower-case letters, only and contains a \"l\"\n",
    "r = r\"[a-z]*l[a-z]*\"\n",
    "\n",
    "# compile regex\n",
    "p = re.compile(r)\n",
    "\n",
    "# create a `Match` object\n",
    "m = p.match(\"hello\")\n",
    "\n",
    "print_result(m)\n",
    "\n",
    "# No upper case letters before l\n",
    "print_result(p.match(\"Hello\"))\n",
    "\n",
    "print_result(p.match(\"hi\"))\n",
    "\n",
    "# After a match is found, the remainder of the string is ignored\n",
    "print_result(p.match(\"hellO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### r before string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello \\n world!\")\n",
    "print(r\"hello \\n world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared brackets indicate a set\n",
    "p = re.compile(r\"[ab]\")\n",
    "\n",
    "print_result(p.match(\"a\"))\n",
    "print_result(p.match(\"b\"))\n",
    "print_result(p.match(\"c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * and +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * matches an arbitrary amount of the preceeding symbol\n",
    "p = re.compile(r\"a*\")\n",
    "\n",
    "print_result(p.match(\"aaaaaa\"))\n",
    "print_result(p.match(\"\"))\n",
    "\n",
    "# + matches an arbitrary amount greater than zero of the preceding symbol\n",
    "p = re.compile(r\"a+\")\n",
    "\n",
    "print_result(p.match(\"aaaaaa\"))\n",
    "print_result(p.match(\"\"))\n",
    "\n",
    "# quantify set\n",
    "p = re.compile(r\"[ab]*c\")\n",
    "\n",
    "print_result(p.match(\"abbbaaabc\"))\n",
    "print_result(p.match(\"bbbbbda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curly brackets can be used for specifying the amount of repetitions\n",
    "p = re.compile(r\"a{5}b\")\n",
    "\n",
    "print_result(p.match(\"aaaaab\"))\n",
    "print_result(p.match(\"aaaab\"))\n",
    "\n",
    "# or a range of repetition amounts\n",
    "p = re.compile(r\"a{4,5}b\")\n",
    "\n",
    "print_result(p.match(\"aaaaab\"))\n",
    "print_result(p.match(\"aaaab\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . matches any symbol, except newline\n",
    "p = re.compile(r\".*y\")\n",
    "\n",
    "print_result(p.match(\"Hi! This is string is matched, as soon as a 'y' is encountered\"))\n",
    "print_result(p.match(\"Newlines\\nbefore 'y' can cause trouble\"))\n",
    "\n",
    "# the `re.DOTALL` flag can help\n",
    "p = re.compile(r\".*y\", re.DOTALL)\n",
    "print_result(p.match(\"Newlines\\nbefore 'y' can cause trouble\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ^ and $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ indicates the start of the string\n",
    "p = re.compile(r\"^hello\")\n",
    "\n",
    "print_result(p.match(\"hello world!\"))\n",
    "print_result(p.match(\"welcome and hello\"))\n",
    "\n",
    "# $ indicates the end of the string\n",
    "p = re.compile(r\".*hello$\")\n",
    "\n",
    "print_result(p.match(\"hello world!\"))\n",
    "print_result(p.match(\"welcome and hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \\ is the escape character\n",
    "p = re.compile(r\"\\*$\")\n",
    "\n",
    "print_result(p.match(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round brackets are used for creating groups\n",
    "p = re.compile(r\".*(hello).*(world)\")\n",
    "m = p.match(\"Somewhere in this text are the words hello and world and some stuff afterwards\")\n",
    "\n",
    "print(m[0]) # the whole match\n",
    "print(m[1]) # first group\n",
    "print(m[2]) # second group\n",
    "\n",
    "# (?P<name>...) can be used for naming groups\n",
    "p = re.compile(r\".*(?P<group01>hello)\")\n",
    "m = p.match(\"The word hello will be found\")\n",
    "\n",
    "m[\"group01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * and + are greedy, they try to match as many symbols as possible\n",
    "p = re.compile(r\".*in\")\n",
    "m = p.match(\"The word 'in' is twice in this sentence\")\n",
    "print(m[0])\n",
    "\n",
    "# use *? for non-greedy behaviour\n",
    "p = re.compile(r\".*?in\")\n",
    "m = p.match(\"The word 'in' is trwice in this sentence\")\n",
    "print(m[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\w and \\s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\w matches any 'word character'\n",
    "p = re.compile(r\"^\\w*$\")\n",
    "print_result(p.match(\"abc_öüäçâбш大水سلام\"))\n",
    "print_result(p.match(\" \"))\n",
    "\n",
    "# \\s matches whitespace characters\n",
    "p = re.compile(r\"^\\s*$\")\n",
    "print_result(p.match(\" \\t\\n\\r\\f\\v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And much more\n",
    "If you come in a situation, where the above is not enough, you can find (much) more syntax in the python `re` [documentation](https://docs.python.org/3/library/re.html#regular-expression-syntax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXJtMRzhzsp5"
   },
   "source": [
    "### Pattern matching\n",
    "https://docs.python.org/3/library/re.html#regular-expression-objects\n",
    "So far we have used ``match()`` to simply find out, if the regex and the string match. Often we rather want to extract substrings with the help of regex. The pattern objects offer several helpful methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\".{10}[Hh]arry.{10}\")\n",
    "\n",
    "# search finds the first occurance of a regex\n",
    "p.search(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findall, well...\n",
    "p.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text to words\n",
    "p = re.compile(r\"\\W+\")\n",
    "\n",
    "p.split(text)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You usually don't have to compile a regular expression. Instead you can put the uncompiled regex as an argument in a function of the ``re`` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r\".{10}[Hh]arry.{10}\", text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\".{10}[Hh]arry.{10}\", text)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR_vZ4tZ0DR1"
   },
   "source": [
    "### Match Objects\n",
    "We have already indexed ``Match`` objects, to find the matched string and used the ``if`` statement, to verify if a regex and a string match. There is more we can do with ``Match`` objects.\n",
    "https://docs.python.org/3/library/re.html#match-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "m = re.match(r\".*?(?P<McG>McGonagall)\", text, re.DOTALL)\n",
    "\n",
    "# start gives the starting index of the specified group in the whole string\n",
    "startpos = m.start(\"McG\")\n",
    "endpos = m.end(\"McG\")\n",
    "print(text[startpos-20:endpos+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupdict can be used to create a dictionary out of named groups\n",
    "m = re.match(r\".*?(?P<first_name>\\w+) (?P<last_name>Dumbledore)\", text, re.DOTALL)\n",
    "print(m.groupdict())\n",
    "m = re.match(r\".*?(?P<first_name>\\w+) (?P<last_name>McGonagall)\", text, re.DOTALL)\n",
    "print(m.groupdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spice up your Soup with Regex\n",
    "When you have extracted some wall of text from a website, you might want to extract certain information out of it with regular expressions. But there are also a few ways of using regex directly in BeautifulSoup functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page = requests.get(\"https://webscrapingtutorial.netlify.app/ids_and_classes.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the tags\n",
    "soup.find_all(re.compile(r'p|(b\\s)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the class\n",
    "print(\"1st try:\", soup.find_all(\"p\", \"-text\"), \"\", sep=\"\\n\")\n",
    "print(\"2nd try:\", soup.find_all(\"p\", re.compile(\"-text\")), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match a tag's attribute\n",
    "soup.find_all(id=re.compile(r\"(first)|(second)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the text instead of tags\n",
    "soup.find_all(string=re.compile(r'\\s(\\w)* outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: What kinds of outer paragraphs are in the tags with class = \"outer...\"?\n",
    "text = str(soup.find_all(\"p\", re.compile(\"outer\")))\n",
    "re.compile(r'\\s(\\w*)\\souter paragraph').findall(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "237750401ff69947b33b76cc1f7b1f2734903febde9cd7f17c4dfcc2f62744fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
